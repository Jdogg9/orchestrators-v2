"""
AIMEE Custom Tokenizer Loader
Auto-generated by AIMEE_Custom_Tiktoken_BPE_Builder_v4
Created: 2026-01-27T06:40:27.940875
"""

import json
import tiktoken
from pathlib import Path
from typing import Optional, Dict, Any

class AIMEETokenizer:
    """Wrapper for AIMEE custom tokenizer"""

    def __init__(self, model_path: Optional[str] = None, meta_path: Optional[str] = None):
        """
        Initialize the AIMEE tokenizer

        Args:
            model_path: Path to .model file (defaults to AIMEE models dir)
            meta_path: Path to metadata JSON (optional)
        """
        if model_path is None:
            model_path = str(Path(__file__).resolve().parent / "gpt-aimee.model")

        if meta_path is None:
            meta_path = str(Path(__file__).resolve().parent / "gpt-aimee_tokenizer_meta.json")
        self.model_path = Path(model_path)
        self.meta_path = Path(meta_path)

        # Load metadata
        self.meta = self._load_meta()

        # Load tiktoken encoding
        self.encoding = self._load_encoding()

    def _load_meta(self) -> Dict[str, Any]:
        """Load tokenizer metadata"""
        if self.meta_path.exists():
            with open(self.meta_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}

    def _load_encoding(self) -> tiktoken.Encoding:
        """Load tiktoken Encoding from tokenizer vocab"""
        # Use the pre-built encoding that was created during training
        # This is more reliable than trying to parse the model file
        import sys
        import os

        # Try to import from the tokenizer module directory
        tokenizer_dir = self.model_path.parent
        if str(tokenizer_dir) not in sys.path:
            sys.path.insert(0, str(tokenizer_dir))

        # Get vocab from current tokenizer if available in globals
        # Otherwise build from saved vocab file if it exists
        vocab_file = self.model_path.parent / f"{self.model_path.stem}_vocab.pkl"
        if vocab_file.exists():
            import pickle
            with open(vocab_file, "rb") as f:
                vocab = pickle.load(f)
                mergeable_ranks = {token_bytes: rank for rank, token_bytes in vocab.items()}
        else:
            # Fallback: use the global encoding if available
            # This works when the loader is imported after training
            raise RuntimeError(
                f"Could not load vocabulary. "
                f"Expected vocab file at: {vocab_file}\n"
                f"Or import this loader while 'enc' is still in globals."
            )

        # Get pattern and special tokens from meta
        pattern = self.meta.get("gpt4_pattern", r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+")
        boundary = self.meta.get("boundary_token", "<|endoftext|>")
        vocab_size = self.meta.get("vocab_size", 50257)

        special_tokens = {boundary: vocab_size}

        return tiktoken.Encoding(
            name="gpt-aimee",
            pat_str=pattern,
            mergeable_ranks=mergeable_ranks,
            special_tokens=special_tokens,
        )

    def encode(self, text: str, allowed_special: str = "all") -> list:
        """Encode text to token IDs"""
        return self.encoding.encode(text, allowed_special=allowed_special)

    def decode(self, tokens: list) -> str:
        """Decode token IDs to text"""
        return self.encoding.decode(tokens)

    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        return len(self.encode(text))

    @property
    def vocab_size(self) -> int:
        """Get vocabulary size"""
        return self.meta.get("vocab_size", 0)

    @property
    def name(self) -> str:
        """Get tokenizer name"""
        return self.meta.get("name", "unknown")

    def info(self) -> Dict[str, Any]:
        """Get tokenizer information"""
        return {
            "name": self.name,
            "vocab_size": self.vocab_size,
            "model_path": str(self.model_path),
            "meta_path": str(self.meta_path),
            "created_at": self.meta.get("aimee_system", {}).get("created_at"),
            "training_chars": self.meta.get("chars_training"),
        }


def load_aimee_tokenizer(model_name: str = "gpt-aimee") -> AIMEETokenizer:
    """
    Load AIMEE tokenizer by name

    Args:
        model_name: Name of the tokenizer model (default: gpt-aimee)

    Returns:
        AIMEETokenizer instance
    """
    tokenizer_dir = Path(__file__).resolve().parent

    model_path = tokenizer_dir / f"{model_name}.model"
    meta_path = tokenizer_dir / f"{model_name}_tokenizer_meta.json"

    if not model_path.exists():
        raise FileNotFoundError(f"Tokenizer model not found: {model_path}")

    return AIMEETokenizer(str(model_path), str(meta_path))


# Example usage
if __name__ == "__main__":
    # Load the tokenizer
    tokenizer = load_aimee_tokenizer()

    # Display info
    print("Tokenizer Info:")
    for k, v in tokenizer.info().items():
        print(f"  {k}: {v}")

    # Test encoding/decoding
    test_text = "Hello AIMEE! This is a test of the custom tokenizer."
    print(f"\nTest text: {test_text}")

    tokens = tokenizer.encode(test_text)
    print(f"Tokens ({len(tokens)}): {tokens[:20]}..." if len(tokens) > 20 else f"Tokens: {tokens}")

    decoded = tokenizer.decode(tokens)
    print(f"Decoded: {decoded}")

    assert decoded == test_text, "Round-trip failed!"
    print("âœ… Round-trip test passed!")
